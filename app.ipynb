{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UltraLightFaceDetecion():\n",
    "    def __init__(self, filepath):\n",
    "        # tflite model init\n",
    "        self._interpreter = tf.lite.Interpreter(model_path=filepath)\n",
    "        self._interpreter.allocate_tensors()\n",
    "\n",
    "\n",
    "    def _pre_processing(self, img):\n",
    "        # resize image to (240,320,3)\n",
    "        resized = cv2.resize(img,dsize=(320,240))\n",
    "        # bgr to rgb\n",
    "        image_rgb = resized[..., ::-1]\n",
    "        # converting values to float type\n",
    "        image_norm = image_rgb.astype(np.float32)\n",
    "        # normalize all pixel values between -1 and 1 (MinMaX)\n",
    "        cv2.normalize(image_norm, image_norm,\n",
    "                      alpha=-1, beta=1, norm_type=cv2.NORM_MINMAX)\n",
    "        # adding another dimension (1, 240, 320, 3)\n",
    "        return image_norm[None, ...]\n",
    "\n",
    "    def inference(self, img):\n",
    "        # BGR image to tensor\n",
    "        input_tensor = self._pre_processing(img)\n",
    "\n",
    "        # get input details, set tensor and invoke\n",
    "        input_details = self._interpreter.get_input_details()\n",
    "        self._interpreter.set_tensor(input_details[0][\"index\"],input_tensor)\n",
    "        self._interpreter.invoke()\n",
    "\n",
    "        # get results (making the inference)\n",
    "        output_details = self._interpreter.get_output_details()\n",
    "        boxes = self._interpreter.get_tensor(output_details[0][\"index\"])\n",
    "        scores = self._interpreter.get_tensor(output_details[1][\"index\"])\n",
    "\n",
    "        # decode boxes to corner format\n",
    "        boxes, scores = self._post_processing(boxes, scores)\n",
    "        # scailing the bounding box according to the aspect ratio\n",
    "        boxes *= np.tile(img.shape[1::-1], 2)\n",
    "        return boxes, scores\n",
    "\n",
    "    def _post_processing(self, boxes, scores):\n",
    "        # bounding box regression\n",
    "        boxes = self._decode_regression(boxes)\n",
    "        # confidencce for every anchor\n",
    "        scores = scores[:, 1]\n",
    "\n",
    "        # confidence threshold filter\n",
    "        conf_mask = 0.6 < scores\n",
    "        # getting all anchors with prob greater then 0.6\n",
    "        boxes, scores = boxes[conf_mask], scores[conf_mask]\n",
    "\n",
    "        # non-maximum suppression\n",
    "        # Prunes away boxes that have high intersection-over-union (IOU) overlap with previously selected boxes\n",
    "        nms_mask = tf.image.non_max_suppression(\n",
    "            boxes, scores, max_output_size=200, iou_threshold=0.3,\n",
    "            score_threshold=float('-inf'), name=None\n",
    "        )\n",
    "        # nms mask contains the indexes of selected boxes\n",
    "        # return boxes that satisfies the requirement \n",
    "        boxes = np.take(boxes, nms_mask, axis=0)\n",
    "        scores = np.take(scores, nms_mask, axis=0)\n",
    "        return boxes, scores\n",
    "\n",
    "    def _decode_regression(self, reg):\n",
    "        # bounding box regression\n",
    "        center_variance = 0.1\n",
    "        size_variance = 0.2\n",
    "\n",
    "        # reading the predifened anchors \n",
    "        with open('./anchors_wh.npy', 'rb') as f:\n",
    "            anchors_wh = np.load(f)\n",
    "        with open('./anchors_xy.npy', 'rb') as f:\n",
    "            anchors_xy = np.load(f)\n",
    "\n",
    "        # mathematical operations\n",
    "        center_xy = reg[:, :2] * center_variance * \\\n",
    "            anchors_wh + anchors_xy\n",
    "        center_wh = np.exp(\n",
    "            reg[:, 2:] * size_variance) * anchors_wh / 2\n",
    "\n",
    "        # center to corner for every anchor\n",
    "        start_xy = center_xy - center_wh\n",
    "        end_xy = center_xy + center_wh\n",
    "        # concatenation of box coordinates\n",
    "        boxes = np.concatenate((start_xy, end_xy), axis=-1)\n",
    "        # clip values of boxes between min 0 max 1\n",
    "        boxes = np.clip(boxes, 0.0, 1.0)\n",
    "        return boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_of_laplacian(image,th=100):\n",
    "\t# calculate this on cropped face\n",
    "\tgray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\tfm = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "\tprint(fm)\n",
    "\tif fm<=th:\n",
    "\t\treturn False\n",
    "\treturn True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ladnmarks(rec,h,w):\n",
    "        landmarks_top = rec[1]\n",
    "        landmarks_bottom = rec[3]\n",
    "        landmarks_left = rec[0]\n",
    "        landmarks_right = rec[2]\n",
    "\n",
    "        # expand bbox\n",
    "        top = int(landmarks_top - 0.8 * (landmarks_bottom - landmarks_top))\n",
    "        bottom = int(landmarks_bottom + 0.3 * (landmarks_bottom - landmarks_top))\n",
    "        left = int(landmarks_left - 0.3 * (landmarks_right - landmarks_left))\n",
    "        right = int(landmarks_right + 0.3 * (landmarks_right - landmarks_left))\n",
    "\n",
    "        if bottom - top > right - left:\n",
    "            left -= ((bottom - top) - (right - left)) // 2\n",
    "            right = left + (bottom - top)\n",
    "        else:\n",
    "            top -= ((right - left) - (bottom - top)) // 2\n",
    "            bottom = top + (right - left)\n",
    "            \n",
    "        left = max(0, left)\n",
    "        right = min(right, w-1)\n",
    "        top = max(0, top)\n",
    "        bottom = min(bottom, h-1)\n",
    "\n",
    "        return (left,top,right,bottom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area(rec1, rec2,h,w):\n",
    "    rec1 = ladnmarks(rec1,h,w)\n",
    "    rec2 = ladnmarks(rec2,h,w)\n",
    "    if (rec1[3] - rec1[1])* (rec1[2] - rec1[0]) <= 0.25*(h*w):\n",
    "        return True\n",
    "    if (rec2[3] - rec2[1])* (rec2[2] - rec2[0]) <= 0.25*(h*w):\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(rec1, rec2,h,w):\n",
    "  rec1 = ladnmarks(rec1,h,w)\n",
    "  rec2 = ladnmarks(rec2,h,w)\n",
    "  if (rec2[2] > rec1[0] and rec2[2] < rec1[2]) or \\\n",
    "     (rec2[0] > rec1[0] and rec2[0] < rec1[2]):\n",
    "    x_match = True\n",
    "  else:\n",
    "    x_match = False\n",
    "  if (rec2[3] > rec1[1] and rec2[3] < rec1[3]) or \\\n",
    "     (rec2[1] > rec1[1] and rec2[1] < rec1[3]):\n",
    "    y_match = True\n",
    "  else:\n",
    "    y_match = False\n",
    "  if x_match and y_match:\n",
    "    return True\n",
    "  else:\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkHuman(boxes,scores,th=.99):\n",
    "    indexes = [idx for idx,score in enumerate(scores) if score<th]\n",
    "    # reverse indexes\n",
    "    indexes = indexes[::-1]\n",
    "    [boxes.pop(i) for i in indexes]\n",
    "    [scores.pop(i) for i in indexes]\n",
    "    return np.array(boxes),np.array(scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_inference(img_path, color=(125, 255, 0)):\n",
    "    # read image\n",
    "    img = cv2.imread(img_path)\n",
    "    quality = True\n",
    "    status = \"Accepted\"\n",
    "    # make inference\n",
    "    boxes, scores = fd.inference(img)\n",
    "    # TODO :: check the threshold \n",
    "    # human face\n",
    "    boxes,scores = checkHuman(list(boxes),list(scores),th=.99)\n",
    "    # No Face\n",
    "    if len(boxes) == 0:\n",
    "        status = \"Rejected:No Face\"\n",
    "    # Multiple Boxes\n",
    "    elif len(boxes) > 2:\n",
    "        status = \"Rejected: Multple Face\"\n",
    "    elif len(boxes) == 2:\n",
    "        if overlap(boxes[0], boxes[1],img.shape[0],img.shape[1]):\n",
    "            status = \"Rejected: Intersecting Faces\"\n",
    "    elif len(boxes) == 2:\n",
    "        if area(boxes[0], boxes[1],img.shape[0],img.shape[1]):\n",
    "            status = \"Rejected: small faces\"\n",
    "    elif quality:\n",
    "        for box,score in zip(boxes.astype(int),scores):\n",
    "            # TODO :: check the threshold \n",
    "            quality = quality and variance_of_laplacian(img[box[1]:box[3],box[0]:box[2]])\n",
    "            # cv2.imwrite(os.path.join(\"./data/output/\", str(score)+img_path.split(\"/\")[-1]),img[box[1]:box[3],box[0]:box[2]])\n",
    "            \n",
    "        if not quality:\n",
    "            status = \"Rejected: Bad Quality\"\n",
    "            \n",
    "    else:\n",
    "        status = \"Accepted\"\n",
    "    # TODO: Reject small face (25%)\n",
    "    # saving the image\n",
    "    cv2.putText(img,status,(0,img.shape[0]-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (209, 255, 0, 255), 1) \n",
    "    cv2.imwrite(os.path.join(\"./data/output/\", img_path.split(\"/\")[-1]),img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All checks\n",
    "# Face not found\n",
    "# human face\n",
    "# Multiple faces\n",
    "# no intersection (accept)\n",
    "# intersection (reject)\n",
    "# Small face\n",
    "# Blurry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFB (higher precision) or slim (faster)\n",
    "mode = 'RFB'\n",
    "filepath = f\"pretrained/version-{mode}-320_without_postprocessing.tflite\"\n",
    "fd = UltraLightFaceDetecion(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,2,4) (4420,2) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16624\\3285485495.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./data/test/test_2021-12-18T06_46_35.891210.jpg'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# test_2021-12-18T06_46_35.891210.jpg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mboxes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_inference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16624\\2544583900.py\u001b[0m in \u001b[0;36mimage_inference\u001b[1;34m(img_path, color)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mstatus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Accepted\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# make inference\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mboxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;31m# TODO :: check the threshold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# human face\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16624\\967658107.py\u001b[0m in \u001b[0;36minference\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# decode boxes to corner format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mboxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[1;31m# scailing the bounding box according to the aspect ratio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mboxes\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16624\\967658107.py\u001b[0m in \u001b[0;36m_post_processing\u001b[1;34m(self, boxes, scores)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_post_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;31m# bounding box regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mboxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decode_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[1;31m# confidencce for every anchor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16624\\967658107.py\u001b[0m in \u001b[0;36m_decode_regression\u001b[1;34m(self, reg)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;31m# mathematical operations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mcenter_xy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mcenter_variance\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0manchors_wh\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0manchors_xy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         center_wh = np.exp(\n\u001b[0;32m     79\u001b[0m             reg[:, 2:] * size_variance) * anchors_wh / 2\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,2,4) (4420,2) "
     ]
    }
   ],
   "source": [
    "# single image inference\n",
    "img_path = './data/test/test_2021-12-18T06_46_35.891210.jpg'\n",
    "# test_2021-12-18T06_46_35.891210.jpg\n",
    "boxes,scores = image_inference(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory inference\n",
    "for img_path in os.listdir(\"./data/test\"):\n",
    "    print(img_path)\n",
    "    img_path = f'./data/test/{img_path}'\n",
    "    image_inference(img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4af60b990e06954d7df26c4fa0aee1452a440eea4e423a15234bd5ab2623fe07"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('facedetect')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
