{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UltraLightFaceDetecion():\n",
    "    def __init__(self, filepath):\n",
    "        # tflite model init\n",
    "        self._interpreter = tf.lite.Interpreter(model_path=filepath)\n",
    "        self._interpreter.allocate_tensors()\n",
    "\n",
    "\n",
    "    def _pre_processing(self, img):\n",
    "        # resize image to (240,320,3)\n",
    "        resized = cv2.resize(img,dsize=(320,240))\n",
    "        # bgr to rgb\n",
    "        image_rgb = resized[..., ::-1]\n",
    "        # converting values to float type\n",
    "        image_norm = image_rgb.astype(np.float32)\n",
    "        # normalize all pixel values between -1 and 1 (MinMaX)\n",
    "        cv2.normalize(image_norm, image_norm,\n",
    "                      alpha=-1, beta=1, norm_type=cv2.NORM_MINMAX)\n",
    "        # adding another dimension (1, 240, 320, 3)\n",
    "        return image_norm[None, ...]\n",
    "\n",
    "    def inference(self, img):\n",
    "        # BGR image to tensor\n",
    "        input_tensor = self._pre_processing(img)\n",
    "\n",
    "        # get input details, set tensor and invoke\n",
    "        input_details = self._interpreter.get_input_details()\n",
    "        self._interpreter.set_tensor(input_details[0][\"index\"],input_tensor)\n",
    "        self._interpreter.invoke()\n",
    "\n",
    "        # get results (making the inference)\n",
    "        output_details = self._interpreter.get_output_details()\n",
    "        boxes = self._interpreter.get_tensor(output_details[0][\"index\"])[0]\n",
    "        scores = self._interpreter.get_tensor(output_details[1][\"index\"])[0]\n",
    "\n",
    "        # decode boxes to corner format\n",
    "        boxes, scores = self._post_processing(boxes, scores)\n",
    "        # scailing the bounding box according to the aspect ratio\n",
    "        boxes *= np.tile(img.shape[1::-1], 2)\n",
    "        return boxes, scores\n",
    "\n",
    "    def _post_processing(self, boxes, scores):\n",
    "        # bounding box regression\n",
    "        boxes = self._decode_regression(boxes)\n",
    "        # confidencce for every anchor\n",
    "        scores = scores[:, 1]\n",
    "\n",
    "        # confidence threshold filter\n",
    "        conf_mask = 0.6 < scores\n",
    "        # getting all anchors with prob greater then 0.6\n",
    "        boxes, scores = boxes[conf_mask], scores[conf_mask]\n",
    "\n",
    "        # non-maximum suppression\n",
    "        # Prunes away boxes that have high intersection-over-union (IOU) overlap with previously selected boxes\n",
    "        nms_mask = tf.image.non_max_suppression(\n",
    "            boxes, scores, max_output_size=200, iou_threshold=0.3,\n",
    "            score_threshold=float('-inf'), name=None\n",
    "        )\n",
    "        # nms mask contains the indexes of selected boxes\n",
    "        # return boxes that satisfies the requirement \n",
    "        boxes = np.take(boxes, nms_mask, axis=0)\n",
    "        scores = np.take(scores, nms_mask, axis=0)\n",
    "        return boxes, scores\n",
    "\n",
    "    def _decode_regression(self, reg):\n",
    "        # bounding box regression\n",
    "        center_variance = 0.1\n",
    "        size_variance = 0.2\n",
    "\n",
    "        # reading the predifened anchors \n",
    "        with open('./anchors_wh.npy', 'rb') as f:\n",
    "            anchors_wh = np.load(f)\n",
    "        with open('./anchors_xy.npy', 'rb') as f:\n",
    "            anchors_xy = np.load(f)\n",
    "\n",
    "        # mathematical operations\n",
    "        center_xy = reg[:, :2] * center_variance * \\\n",
    "            anchors_wh + anchors_xy\n",
    "        center_wh = np.exp(\n",
    "            reg[:, 2:] * size_variance) * anchors_wh / 2\n",
    "\n",
    "        # center to corner for every anchor\n",
    "        start_xy = center_xy - center_wh\n",
    "        end_xy = center_xy + center_wh\n",
    "        # concatenation of box coordinates\n",
    "        boxes = np.concatenate((start_xy, end_xy), axis=-1)\n",
    "        # clip values of boxes between min 0 max 1\n",
    "        boxes = np.clip(boxes, 0.0, 1.0)\n",
    "        return boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_of_laplacian(image,th=100):\n",
    "\t# calculate this on cropped face\n",
    "\tgray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\tfm = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "\tif fm<=th:\n",
    "\t\treturn False\n",
    "\treturn True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ladnmarks(rec,h,w):\n",
    "        landmarks_top = rec[1]\n",
    "        landmarks_bottom = rec[3]\n",
    "        landmarks_left = rec[0]\n",
    "        landmarks_right = rec[2]\n",
    "\n",
    "        # expand bbox\n",
    "        top = int(landmarks_top - 0.8 * (landmarks_bottom - landmarks_top))\n",
    "        bottom = int(landmarks_bottom + 0.3 * (landmarks_bottom - landmarks_top))\n",
    "        left = int(landmarks_left - 0.3 * (landmarks_right - landmarks_left))\n",
    "        right = int(landmarks_right + 0.3 * (landmarks_right - landmarks_left))\n",
    "\n",
    "        if bottom - top > right - left:\n",
    "            left -= ((bottom - top) - (right - left)) // 2\n",
    "            right = left + (bottom - top)\n",
    "        else:\n",
    "            top -= ((right - left) - (bottom - top)) // 2\n",
    "            bottom = top + (right - left)\n",
    "\n",
    "        left = max(0, left)\n",
    "        right = min(right, w-1)\n",
    "        top = max(0, top)\n",
    "        bottom = min(bottom, h-1)\n",
    "\n",
    "        return (left,top,right,bottom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap(rec1, rec2,h,w):\n",
    "  rec1 = ladnmarks(rec1,h,w)\n",
    "  rec2 = ladnmarks(rec2,h,w)\n",
    "  if (rec2[2] > rec1[0] and rec2[2] < rec1[2]) or \\\n",
    "     (rec2[0] > rec1[0] and rec2[0] < rec1[2]):\n",
    "    x_match = True\n",
    "  else:\n",
    "    x_match = False\n",
    "  if (rec2[3] > rec1[1] and rec2[3] < rec1[3]) or \\\n",
    "     (rec2[1] > rec1[1] and rec2[1] < rec1[3]):\n",
    "    y_match = True\n",
    "  else:\n",
    "    y_match = False\n",
    "  if x_match and y_match:\n",
    "    return True\n",
    "  else:\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkHuman(boxes,scores,th=.99):\n",
    "    indexes = [idx for idx,score in enumerate(scores) if score<th]\n",
    "    # reverse indexes\n",
    "    indexes = indexes[::-1]\n",
    "    [boxes.pop(i) for i in indexes]\n",
    "    [scores.pop(i) for i in indexes]\n",
    "    return np.array(boxes),np.array(scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_inference(img_path, color=(125, 255, 0)):\n",
    "    # read image\n",
    "    img = cv2.imread(img_path)\n",
    "    quality = True\n",
    "    status = \"Accepted\"\n",
    "    # make inference\n",
    "    boxes, scores = fd.inference(img)\n",
    "    # human face\n",
    "    # TODO :: check the threshold \n",
    "    boxes,scores = checkHuman(list(boxes),list(scores),th=.99)\n",
    "    # No Face\n",
    "    if len(boxes) == 0:\n",
    "        status = \"Rejected:No Face\"\n",
    "    # Multiple Boxes\n",
    "    elif len(boxes) > 2:\n",
    "        status = \"Rejected: Multple Face\"\n",
    "    elif len(boxes) == 2:\n",
    "        # TODO: Add padding to the bounding box\n",
    "        if overlap(boxes[0], boxes[1],img.shape[0],img.shape[1]):\n",
    "            # box1 = ladnmarks(boxes[0],img.shape[0],img.shape[1])\n",
    "            # box2 = ladnmarks(boxes[1],img.shape[0],img.shape[1])\n",
    "            # print(box1)\n",
    "            # print(box2)\n",
    "            # cv2.imwrite(os.path.join(\"./data/output/\", \"1\"+img_path.split(\"/\")[-1]),img[box1[1]:box1[3],box1[0]:box1[2]])\n",
    "            # cv2.imwrite(os.path.join(\"./data/output/\", \"2\"+img_path.split(\"/\")[-1]),img[box2[1]:box2[3],box2[0]:box2[2]])\n",
    "            status = \"Rejected: Intersecting Faces\"\n",
    "    elif quality:\n",
    "        for box,score in zip(boxes.astype(int),scores):\n",
    "            # TODO :: check the threshold \n",
    "            quality = quality and variance_of_laplacian(img[box[1]:box[3],box[0]:box[2]])\n",
    "            # cv2.imwrite(os.path.join(\"./data/output/\", str(score)+img_path.split(\"/\")[-1]),img[box[1]:box[3],box[0]:box[2]])\n",
    "            \n",
    "        if not quality:\n",
    "            status = \"Rejected: Bad Quality\"\n",
    "    else:\n",
    "        status = \"Accepted\"\n",
    "    # TODO: Reject small face (25%)\n",
    "    # saving the image\n",
    "    cv2.putText(img,status,(0,img.shape[0]-5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (209, 255, 0, 255), 1) \n",
    "    cv2.imwrite(os.path.join(\"./data/output/\", img_path.split(\"/\")[-1]),img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All checks\n",
    "# Face not found\n",
    "# human face\n",
    "# Multiple faces\n",
    "# no intersection (accept)\n",
    "# intersection (reject)\n",
    "# Small face\n",
    "# Blurry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFB (higher precision) or slim (faster)\n",
    "mode = 'RFB'\n",
    "filepath = f\"pretrained/version-{mode}-320_without_postprocessing.tflite\"\n",
    "fd = UltraLightFaceDetecion(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single image inference\n",
    "img_path = './data/test/test_2021-12-18T06_46_35.891210.jpg'\n",
    "# test_2021-12-18T06_46_35.891210.jpg\n",
    "image_inference(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name :  0d9b0714544352fb74b198a05e4c955e.jpg\n",
      "313.9582700890112\n",
      "Name :  2022-01-01_135.png\n",
      "1250.865387414949\n",
      "Name :  2022-01-01_192.png\n",
      "151.09068701672956\n",
      "Name :  2022-01-01_206.png\n",
      "423.85632379058023\n",
      "Name :  2022-01-01_232.png\n",
      "396.1223907272918\n",
      "Name :  2022-01-01_268.png\n",
      "127.53937064961109\n",
      "Name :  2022-01-01_3.jpg\n",
      "156.49139257131523\n",
      "Name :  2022-01-01_30.jpg\n",
      "75.45239572959008\n",
      "Name :  2022-01-01_312.png\n",
      "60.04138272134124\n",
      "Name :  2022-01-01_33.jpg\n",
      "466.0931542450148\n",
      "Name :  2022-01-01_337.png\n",
      "264.69623416568\n",
      "Name :  2022-01-01_340.png\n",
      "298.0011827118431\n",
      "Name :  2022-01-01_39.jpg\n",
      "157.50535197609787\n",
      "Name :  2022-01-01_426.png\n",
      "915.2644946977967\n",
      "Name :  2022-01-01_44.jpg\n",
      "37.618307501218716\n",
      "Name :  2022-01-01_440.png\n",
      "74.87428725465139\n",
      "Name :  2022-01-01_46.jpg\n",
      "276.64996290012016\n",
      "Name :  2022-01-01_491.png\n",
      "107.56267824977492\n",
      "Name :  2022-01-01_539.png\n",
      "899.5416355133451\n",
      "Name :  2022-01-01_54.jpg\n",
      "64.2518306721946\n",
      "Name :  2022-01-01_55.jpg\n",
      "402.3639867935299\n",
      "Name :  2022-01-01_58.jpg\n",
      "246.7819785123361\n",
      "Name :  2022-01-01_59.jpg\n",
      "70.22220908701104\n",
      "Name :  2022-01-01_599.png\n",
      "573.149335299683\n",
      "Name :  2022-01-01_602.png\n",
      "115.11999918772138\n",
      "Name :  2022-01-01_668.png\n",
      "332.04952784535766\n",
      "Name :  2022-01-01_682.png\n",
      "564.6818875953901\n",
      "Name :  2022-01-01_77.jpg\n",
      "207.50830706193915\n",
      "Name :  2022-01-01_78.jpg\n",
      "141.79963671321732\n",
      "Name :  2022-01-01_8.jpg\n",
      "415.57681118456867\n",
      "Name :  2022-01-01_83.jpg\n",
      "61.00542747908439\n",
      "Name :  2022-01-01_88.jpg\n",
      "1089.6128923747556\n",
      "Name :  2022-01-01_94.png\n",
      "355.54177048289426\n",
      "Name :  2022-02-14_448.jpg\n",
      "110.14406670072563\n",
      "Name :  2022-02-14_548.jpg\n",
      "140.70260669483974\n",
      "Name :  2022-02-14_653.jpg\n",
      "956.7318116299764\n",
      "Name :  2022-02-15_3837.jpg\n",
      "77.43978540946814\n",
      "Name :  2022-02-15_4067.jpg\n",
      "380.750821366505\n",
      "Name :  download.jpeg\n",
      "1736.7324816166233\n",
      "Name :  indian-4238750_1280.jpg\n",
      "3818.391152800288\n",
      "Name :  indian-boy-with-messy-hair.jpg\n",
      "1143.6364629177542\n",
      "Name :  test_2021-12-18T00_31_45.085918.jpg\n",
      "407.4499435343907\n",
      "Name :  test_2021-12-18T00_34_43.924920.jpg\n",
      "56.72425930475438\n",
      "Name :  test_2021-12-18T00_45_57.825562.jpg\n",
      "Name :  test_2021-12-18T00_51_12.514570.jpg\n",
      "92.16535531065368\n",
      "Name :  test_2021-12-18T00_54_34.062274.jpg\n",
      "288.5935246761136\n",
      "Name :  test_2021-12-18T01_26_58.877964.jpg\n",
      "Name :  test_2021-12-18T01_56_19.137959.jpg\n",
      "54.64865446089552\n",
      "Name :  test_2021-12-18T02_01_59.913697.jpg\n",
      "180.43210149169343\n",
      "Name :  test_2021-12-18T02_08_05.625370.jpg\n",
      "96.52799979136344\n",
      "Name :  test_2021-12-18T02_18_29.079487.jpg\n",
      "474.10806486857285\n",
      "Name :  test_2021-12-18T02_42_47.046216.jpg\n",
      "585.058078938026\n",
      "Name :  test_2021-12-18T03_35_55.797170.jpg\n",
      "232.15203232202322\n",
      "Name :  test_2021-12-18T03_37_37.552865.jpg\n",
      "132.7993479523555\n",
      "Name :  test_2021-12-18T03_50_39.134813.jpg\n",
      "Name :  test_2021-12-18T03_57_12.522557.jpg\n",
      "41.70894520542835\n",
      "Name :  test_2021-12-18T04_09_43.877735.jpg\n",
      "450.8073290799793\n",
      "Name :  test_2021-12-18T04_45_19.021288.jpg\n",
      "629.7642491447698\n",
      "Name :  test_2021-12-18T05_04_00.458065.jpg\n",
      "229.7209827743239\n",
      "Name :  test_2021-12-18T05_19_14.729515.jpg\n",
      "Name :  test_2021-12-18T06_12_59.806770.jpg\n",
      "23.86917040856162\n",
      "Name :  test_2021-12-18T06_19_08.613002.jpg\n",
      "2850.2470508931906\n",
      "Name :  test_2021-12-18T06_27_11.451196.jpg\n",
      "Name :  test_2021-12-18T06_31_29.735128.jpg\n",
      "33.60266133249082\n",
      "Name :  test_2021-12-18T06_46_35.891210.jpg\n",
      "Name :  test_2021-12-18T07_01_01.354214.jpg\n",
      "397.96467978660127\n",
      "Name :  test_2021-12-18T07_08_44.646969.jpg\n",
      "825.6002866932664\n",
      "Name :  test_2021-12-18T07_52_09.413836.jpg\n",
      "1253.4696965421267\n",
      "Name :  test_2021-12-18T08_15_37.476707.jpg\n",
      "220.86294803877647\n",
      "Name :  test_2021-12-18T08_50_02.023429.jpg\n",
      "Name :  test_2021-12-19T00_08_23.743340.jpg\n",
      "455.54472174344875\n",
      "Name :  test_2021-12-19T00_10_36.231287.jpg\n",
      "11.505513951117727\n",
      "Name :  test_2022-02-13T01_13_41.025572.jpg\n",
      "243.95056499922234\n"
     ]
    }
   ],
   "source": [
    "# directory inference\n",
    "for img_path in os.listdir(\"./data/input\"):\n",
    "    img_path = f'./data/input/{img_path}'\n",
    "    image_inference(img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4af60b990e06954d7df26c4fa0aee1452a440eea4e423a15234bd5ab2623fe07"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('facedetect')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
